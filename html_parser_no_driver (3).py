# ============================================
# ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ä¸è¦ HTMLè§£æãƒ„ãƒ¼ãƒ«
# BeautifulSoup + requests ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
# Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ä¸è¦ã§è¶…é«˜é€Ÿï¼
# ============================================

# --- å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
import requests  # HTTPé€šä¿¡ã§Webãƒšãƒ¼ã‚¸ã‚’å–å¾—ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from bs4 import BeautifulSoup  # HTMLã‚’è§£æã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆBeautifulSoup4ï¼‰
import logging  # ãƒ­ã‚°å‡ºåŠ›ç”¨ã®æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from datetime import datetime  # ç¾åœ¨æ—¥æ™‚ã®å–å¾—ç”¨
from typing import List, Dict, Optional # ============================================
# ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ä¸è¦ HTMLè§£æãƒ„ãƒ¼ãƒ«
# BeautifulSoup + requests ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
# ============================================

import requests  # HTTPé€šä¿¡ç”¨
from bs4 import BeautifulSoup  # HTMLè§£æç”¨
import logging  # ãƒ­ã‚°å‡ºåŠ›ç”¨
from datetime import datetime  # æ—¥æ™‚å–å¾—ç”¨
from typing import List, Dict, Optional  # å‹ãƒ’ãƒ³ãƒˆç”¨
from urllib.parse import urljoin, urlparse  # URLå‡¦ç†ç”¨
import json  # JSONå‡ºåŠ›ç”¨
import csv  # CSVå‡ºåŠ›ç”¨


# ============================================
# ãƒ­ã‚°è¨­å®š
# ============================================

logging.basicConfig(
    level=logging.DEBUG,  # DEBUGãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã‚’å‡ºåŠ›
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('html_analysis.log', encoding='utf-8'),  # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        logging.StreamHandler()  # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚è¡¨ç¤º
    ]
)

logger = logging.getLogger(__name__)


# ============================================
# HTMLè§£æã‚¯ãƒ©ã‚¹
# ============================================

class HTMLAnalyzer:
    """
    ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ä¸è¦ã§HTMLã‚’è§£æã™ã‚‹ã‚¯ãƒ©ã‚¹
    requests + BeautifulSoup ã‚’ä½¿ç”¨
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        print("[DEBUG] HTMLAnalyzerã‚’åˆæœŸåŒ–")
        logger.info("HTMLAnalyzeråˆæœŸåŒ–")
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®šï¼ˆãƒœãƒƒãƒˆæ¤œå‡ºã‚’å›é¿ï¼‰
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                         '(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        
        self.soup = None  # BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä¿å­˜
        self.url = None  # ç¾åœ¨ã®URL
        self.html = None  # HTMLæ–‡å­—åˆ—
    
    def fetch_url(self, url: str, timeout: int = 10) -> bool:
        """
        URLã‹ã‚‰HTMLã‚’å–å¾—
        
        Args:
            url: å–å¾—ã™ã‚‹URL
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰
            
        Returns:
            æˆåŠŸæ™‚Trueã€å¤±æ•—æ™‚False
        """
        print(f"\n[DEBUG] URLã«ã‚¢ã‚¯ã‚»ã‚¹: {url}")
        logger.info(f"URLå–å¾—é–‹å§‹: {url}")
        
        try:
            # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
            response = requests.get(url, headers=self.headers, timeout=timeout)
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèª
            print(f"[DEBUG] ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
            logger.debug(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
            
            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            response.raise_for_status()  # 4xx, 5xxã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ä¾‹å¤–ã‚’ç™ºç”Ÿ
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¨­å®šï¼ˆæ–‡å­—åŒ–ã‘å¯¾ç­–ï¼‰
            if response.encoding == 'ISO-8859-1':  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å ´åˆ
                response.encoding = response.apparent_encoding  # è‡ªå‹•æ¤œå‡º
            
            # HTMLã‚’ä¿å­˜
            self.html = response.text
            self.url = url
            
            # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
            self.soup = BeautifulSoup(self.html, 'html.parser')
            
            print(f"[DEBUG] âœ… HTMLå–å¾—æˆåŠŸ")
            print(f"[DEBUG] HTMLé•·: {len(self.html):,} æ–‡å­—")
            print(f"[DEBUG] ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {response.encoding}")
            
            logger.info(f"HTMLå–å¾—æˆåŠŸ: {len(self.html)}æ–‡å­—")
            logger.debug(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {response.encoding}")
            
            return True
            
        except requests.exceptions.Timeout:
            print(f"[DEBUG] âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout}ç§’ä»¥å†…ã«å¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            logger.error(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}")
            return False
            
        except requests.exceptions.HTTPError as e:
            print(f"[DEBUG] âŒ HTTPã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"HTTPã‚¨ãƒ©ãƒ¼: {e}")
            return False
            
        except requests.exceptions.RequestException as e:
            print(f"[DEBUG] âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False
    
    def load_from_file(self, filepath: str) -> bool:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            filepath: HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            æˆåŠŸæ™‚Trueã€å¤±æ•—æ™‚False
        """
        print(f"\n[DEBUG] ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿: {filepath}")
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {filepath}")
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
            with open(filepath, 'r', encoding='utf-8') as f:
                self.html = f.read()
            
            # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
            self.soup = BeautifulSoup(self.html, 'html.parser')
            self.url = f"file://{filepath}"
            
            print(f"[DEBUG] âœ… ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
            print(f"[DEBUG] HTMLé•·: {len(self.html):,} æ–‡å­—")
            
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {len(self.html)}æ–‡å­—")
            
            return True
            
        except FileNotFoundError:
            print(f"[DEBUG] âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«æœªç™ºè¦‹: {filepath}")
            return False
            
        except Exception as e:
            print(f"[DEBUG] âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return False
    
    def get_page_info(self) -> Dict[str, any]:
        """
        ãƒšãƒ¼ã‚¸ã®åŸºæœ¬æƒ…å ±ã‚’å–å¾—
        
        Returns:
            ãƒšãƒ¼ã‚¸æƒ…å ±ã®è¾æ›¸
        """
        if not self.soup:
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            logger.warning("HTMLæœªèª­ã¿è¾¼ã¿")
            return {}
        
        print("\n[DEBUG] ========== ãƒšãƒ¼ã‚¸åŸºæœ¬æƒ…å ± ==========")
        logger.info("ãƒšãƒ¼ã‚¸æƒ…å ±å–å¾—é–‹å§‹")
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
        title = self.soup.title.string if self.soup.title else "(ã‚¿ã‚¤ãƒˆãƒ«ãªã—)"
        
        # ãƒ¡ã‚¿æƒ…å ±ã‚’å–å¾—
        description = ""
        keywords = ""
        
        # descriptionãƒ¡ã‚¿ã‚¿ã‚°
        meta_desc = self.soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            description = meta_desc.get('content', '')
        
        # keywordsãƒ¡ã‚¿ã‚¿ã‚°
        meta_keywords = self.soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords:
            keywords = meta_keywords.get('content', '')
        
        info = {
            'url': self.url,
            'title': title,
            'description': description,
            'keywords': keywords,
            'html_length': len(self.html)
        }
        
        # æƒ…å ±ã‚’è¡¨ç¤º
        print(f"[DEBUG] URL: {info['url']}")
        print(f"[DEBUG] ã‚¿ã‚¤ãƒˆãƒ«: {info['title']}")
        print(f"[DEBUG] èª¬æ˜: {info['description'][:100] if info['description'] else '(ãªã—)'}...")
        print(f"[DEBUG] ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {info['keywords'][:100] if info['keywords'] else '(ãªã—)'}...")
        print(f"[DEBUG] HTMLé•·: {info['html_length']:,} æ–‡å­—")
        
        logger.info(f"ãƒšãƒ¼ã‚¸æƒ…å ±: {title}")
        logger.debug(f"HTMLé•·: {info['html_length']}")
        
        return info
    
    def analyze_structure(self) -> Dict[str, int]:
        """
        HTMLæ§‹é€ ã‚’åˆ†æï¼ˆè¦ç´ æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼‰
        
        Returns:
            è¦ç´ æ•°ã®è¾æ›¸
        """
        if not self.soup:
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return {}
        
        print("\n[DEBUG] ========== HTMLæ§‹é€ åˆ†æ ==========")
        logger.info("æ§‹é€ åˆ†æé–‹å§‹")
        
        # ä¸»è¦ãªè¦ç´ ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        elements = [
            'div', 'span', 'p', 'a', 'img', 'table', 'tr', 'td',
            'ul', 'ol', 'li', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6',
            'form', 'input', 'button', 'select', 'textarea',
            'nav', 'header', 'footer', 'section', 'article', 'aside'
        ]
        
        counts = {}
        
        print("[DEBUG] è¦ç´ æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆä¸­...")
        
        for element in elements:
            # è¦ç´ ã‚’æ¤œç´¢ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆ
            found = self.soup.find_all(element)
            count = len(found)
            counts[element] = count
            
            if count > 0:  # å­˜åœ¨ã™ã‚‹è¦ç´ ã®ã¿è¡¨ç¤º
                print(f"[DEBUG]   <{element}>: {count}å€‹")
                logger.debug(f"è¦ç´ : <{element}> = {count}")
        
        logger.info(f"æ§‹é€ åˆ†æå®Œäº†: {len(counts)}ç¨®é¡ã®è¦ç´ ")
        
        return counts
    
    def find_by_class(self, class_name: str) -> List:
        """
        ã‚¯ãƒ©ã‚¹åã§è¦ç´ ã‚’æ¤œç´¢
        
        Args:
            class_name: æ¤œç´¢ã™ã‚‹ã‚¯ãƒ©ã‚¹å
            
        Returns:
            è¦‹ã¤ã‹ã£ãŸè¦ç´ ã®ãƒªã‚¹ãƒˆ
        """
        if not self.soup:
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        print(f"\n[DEBUG] ã‚¯ãƒ©ã‚¹åã§æ¤œç´¢: '{class_name}'")
        logger.info(f"ã‚¯ãƒ©ã‚¹æ¤œç´¢: {class_name}")
        
        # ã‚¯ãƒ©ã‚¹åã§æ¤œç´¢
        elements = self.soup.find_all(class_=class_name)
        
        print(f"[DEBUG] âœ… {len(elements)}å€‹ã®è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        logger.info(f"ã‚¯ãƒ©ã‚¹æ¤œç´¢çµæœ: {len(elements)}å€‹")
        
        # å„è¦ç´ ã®æƒ…å ±ã‚’è¡¨ç¤º
        for i, element in enumerate(elements[:5], 1):  # æœ€åˆã®5å€‹
            text = element.get_text(strip=True)[:50]
            tag = element.name
            print(f"[DEBUG]   [{i}] <{tag}> {text}...")
            logger.debug(f"è¦ç´ [{i}]: <{tag}> {text[:30]}")
        
        if len(elements) > 5:
            print(f"[DEBUG]   ... ä»– {len(elements) - 5}å€‹")
        
        return elements
    
    def find_by_id(self, element_id: str):
        """
        IDã§è¦ç´ ã‚’æ¤œç´¢
        
        Args:
            element_id: æ¤œç´¢ã™ã‚‹è¦ç´ ã®ID
            
        Returns:
            è¦‹ã¤ã‹ã£ãŸè¦ç´ ã¾ãŸã¯None
        """
        if not self.soup:
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
        
        print(f"\n[DEBUG] IDã§æ¤œç´¢: '{element_id}'")
        logger.info(f"IDæ¤œç´¢: {element_id}")
        
        # IDã§æ¤œç´¢
        element = self.soup.find(id=element_id)
        
        if element:
            text = element.get_text(strip=True)[:100]
            tag = element.name
            print(f"[DEBUG] âœ… è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            print(f"[DEBUG]   ã‚¿ã‚°: <{tag}>")
            print(f"[DEBUG]   ãƒ†ã‚­ã‚¹ãƒˆ: {text}...")
            logger.info(f"IDæ¤œç´¢æˆåŠŸ: {element_id}")
        else:
            print(f"[DEBUG] âŒ è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            logger.warning(f"IDæ¤œç´¢å¤±æ•—: {element_id}")
        
        return element
    
    def find_by_tag(self, tag_name: str) -> List:
        """
        ã‚¿ã‚°åã§è¦ç´ ã‚’æ¤œç´¢
        
        Args:
            tag_name: æ¤œç´¢ã™ã‚‹ã‚¿ã‚°å
            
        Returns:
            è¦‹ã¤ã‹ã£ãŸè¦ç´ ã®ãƒªã‚¹ãƒˆ
        """
        if not self.soup:
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        print(f"\n[DEBUG] ã‚¿ã‚°åã§æ¤œç´¢: '<{tag_name}>'")
        logger.info(f"ã‚¿ã‚°æ¤œç´¢: {tag_name}")
        
        # ã‚¿ã‚°åã§æ¤œç´¢
        elements = self.soup.find_all(tag_name)
        
        print(f"[DEBUG] âœ… {len(elements)}å€‹ã®è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        logger.info(f"ã‚¿ã‚°æ¤œç´¢çµæœ: {len(elements)}å€‹")
        
        return elements
    
    def find_by_css_selector(self, selector: str) -> List:
        """
        CSSã‚»ãƒ¬ã‚¯ã‚¿ã§è¦ç´ ã‚’æ¤œç´¢
        
        Args:
            selector: CSSã‚»ãƒ¬ã‚¯ã‚¿
            
        Returns:
            è¦‹ã¤ã‹ã£ãŸè¦ç´ ã®ãƒªã‚¹ãƒˆ
        """
        if not self.soup:
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        print(f"\n[DEBUG] CSSã‚»ãƒ¬ã‚¯ã‚¿ã§æ¤œç´¢: '{selector}'")
        logger.info(f"CSSæ¤œç´¢: {selector}")
        
        # CSSã‚»ãƒ¬ã‚¯ã‚¿ã§æ¤œç´¢
        elements = self.soup.select(selector)
        
        print(f"[DEBUG] âœ… {len(elements)}å€‹ã®è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        logger.info(f"CSSæ¤œç´¢çµæœ: {len(elements)}å€‹")
        
        # å„è¦ç´ ã®æƒ…å ±ã‚’è¡¨ç¤º
        for i, element in enumerate(elements[:5], 1):
            text = element.get_text(strip=True)[:50]
            tag = element.name
            print(f"[DEBUG]   [{i}] <{tag}> {text}...")
            logger.debug(f"è¦ç´ [{i}]: <{tag}> {text[:30]}")
        
        if len(elements) > 5:
            print(f"[DEBUG]   ... ä»– {len(elements) - 5}å€‹")
        
        return elements
    
    def get_all_links(self) -> List[Dict[str, str]]:
        """
        ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—
        
        Returns:
            ãƒªãƒ³ã‚¯æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        if not self.soup:
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        print("\n[DEBUG] ========== ãƒªãƒ³ã‚¯ä¸€è¦§ ==========")
        logger.info("ãƒªãƒ³ã‚¯å–å¾—é–‹å§‹")
        
        # ã™ã¹ã¦ã®<a>ã‚¿ã‚°ã‚’æ¤œç´¢
        links = self.soup.find_all('a')
        
        link_data = []
        
        print(f"[DEBUG] {len(links)}å€‹ã®ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        logger.info(f"ãƒªãƒ³ã‚¯æ•°: {len(links)}")
        
        # å„ãƒªãƒ³ã‚¯ã®æƒ…å ±ã‚’å–å¾—
        for i, link in enumerate(links[:10], 1):  # æœ€åˆã®10å€‹
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            if href:
                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                absolute_url = urljoin(self.url, href) if self.url else href
                
                link_info = {
                    'href': absolute_url,
                    'text': text if text else '(ãƒ†ã‚­ã‚¹ãƒˆãªã—)',
                    'original_href': href
                }
                link_data.append(link_info)
                
                print(f"[DEBUG]   [{i}] {link_info['text'][:40]}")
                print(f"[DEBUG]       â†’ {absolute_url}")
                logger.debug(f"ãƒªãƒ³ã‚¯[{i}]: {text[:30]} -> {href}")
        
        if len(links) > 10:
            print(f"[DEBUG]   ... ä»– {len(links) - 10}å€‹")
        
        logger.info(f"ãƒªãƒ³ã‚¯å–å¾—å®Œäº†: {len(link_data)}å€‹")
        
        return link_data
    
    def get_all_images(self) -> List[Dict[str, str]]:
        """
        ã™ã¹ã¦ã®ç”»åƒã‚’å–å¾—
        
        Returns:
            ç”»åƒæƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        if not self.soup:
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        print("\n[DEBUG] ========== ç”»åƒä¸€è¦§ ==========")
        logger.info("ç”»åƒå–å¾—é–‹å§‹")
        
        # ã™ã¹ã¦ã®<img>ã‚¿ã‚°ã‚’æ¤œç´¢
        images = self.soup.find_all('img')
        
        image_data = []
        
        print(f"[DEBUG] {len(images)}å€‹ã®ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        logger.info(f"ç”»åƒæ•°: {len(images)}")
        
        # å„ç”»åƒã®æƒ…å ±ã‚’å–å¾—
        for i, img in enumerate(images[:10], 1):  # æœ€åˆã®10å€‹
            src = img.get('src', '')
            alt = img.get('alt', '')
            
            if src:
                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                absolute_url = urljoin(self.url, src) if self.url else src
                
                img_info = {
                    'src': absolute_url,
                    'alt': alt if alt else '(altãªã—)',
                    'original_src': src
                }
                image_data.append(img_info)
                
                print(f"[DEBUG]   [{i}] alt='{img_info['alt'][:40]}'")
                print(f"[DEBUG]       src: {absolute_url[:60]}...")
                logger.debug(f"ç”»åƒ[{i}]: alt={alt} -> {src[:50]}")
        
        if len(images) > 10:
            print(f"[DEBUG]   ... ä»– {len(images) - 10}å€‹")
        
        logger.info(f"ç”»åƒå–å¾—å®Œäº†: {len(image_data)}å€‹")
        
        return image_data
    
    def save_html(self, filename: Optional[str] = None):
        """
        HTMLã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        
        Args:
            filename: ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        if not self.html:
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'saved_html_{timestamp}.html'
        
        print(f"\n[DEBUG] HTMLã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜: {filename}")
        logger.info(f"HTMLä¿å­˜: {filename}")
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(self.html)
            
            print(f"[DEBUG] âœ… ä¿å­˜æˆåŠŸ: {len(self.html):,} æ–‡å­—")
            logger.info(f"HTMLä¿å­˜å®Œäº†: {len(self.html)}æ–‡å­—")
            
        except Exception as e:
            print(f"[DEBUG] âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"HTMLä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def extract_text(self) -> str:
        """
        HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’æŠ½å‡º
        
        Returns:
            ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—
        """
        if not self.soup:
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return ""
        
        print("\n[DEBUG] ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºä¸­...")
        logger.info("ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºé–‹å§‹")
        
        # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å–å¾—
        text = self.soup.get_text(separator='\n', strip=True)
        
        print(f"[DEBUG] âœ… ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†: {len(text):,} æ–‡å­—")
        logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†: {len(text)}æ–‡å­—")
        
        return text
    
    def pretty_print(self):
        """HTMLã‚’æ•´å½¢ã—ã¦è¡¨ç¤º"""
        if not self.soup:
            print("[DEBUG] âŒ HTMLãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        print("\n[DEBUG] ========== æ•´å½¢HTML ==========")
        
        # prettify()ã§æ•´å½¢
        pretty_html = self.soup.prettify()
        
        # æœ€åˆã®50è¡Œã®ã¿è¡¨ç¤º
        lines = pretty_html.split('\n')[:50]
        print('\n'.join(lines))
        
        if len(pretty_html.split('\n')) > 50:
            print(f"\n... ä»– {len(pretty_html.split('\n')) - 50}è¡Œ")


# ============================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
# ============================================

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("=" * 70)
    print("ğŸ” ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ä¸è¦ HTMLè§£æãƒ„ãƒ¼ãƒ«")
    print("=" * 70)
    logger.info("ãƒ—ãƒ­ã‚°ãƒ©ãƒ é–‹å§‹")
    
    analyzer = HTMLAnalyzer()
    
    print("\nè§£ææ–¹æ³•ã‚’é¸æŠ:")
    print("1. URLã‹ã‚‰å–å¾—")
    print("2. ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿")
    
    choice = input("\né¸æŠ (1-2): ")
    
    try:
        if choice == "1":
            # URLã‹ã‚‰å–å¾—
            url = input("URL: ")
            if not analyzer.fetch_url(url):
                print("[DEBUG] âŒ HTMLå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
        
        elif choice == "2":
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
            filepath = input("HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹: ")
            if not analyzer.load_from_file(filepath):
                print("[DEBUG] âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
        
        else:
            print("[DEBUG] âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
            return
        
        # ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’å–å¾—
        page_info = analyzer.get_page_info()
        
        # æ§‹é€ ã‚’åˆ†æ
        structure = analyzer.analyze_structure()
        
        # ãƒªãƒ³ã‚¯ä¸€è¦§
        links = analyzer.get_all_links()
        
        # ç”»åƒä¸€è¦§
        images = analyzer.get_all_images()
        
        # HTMLã‚’ä¿å­˜
        analyzer.save_html()
        
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
        print("\n" + "=" * 70)
        print("ğŸ“‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
        print("=" * 70)
        print("ã‚³ãƒãƒ³ãƒ‰:")
        print("  class <ã‚¯ãƒ©ã‚¹å>  - ã‚¯ãƒ©ã‚¹ã§æ¤œç´¢")
        print("  id <ID>          - IDã§æ¤œç´¢")
        print("  tag <ã‚¿ã‚°å>      - ã‚¿ã‚°ã§æ¤œç´¢")
        print("  css <ã‚»ãƒ¬ã‚¯ã‚¿>    - CSSã‚»ãƒ¬ã‚¯ã‚¿ã§æ¤œç´¢")
        print("  text             - ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º")
        print("  quit             - çµ‚äº†")
        
        while True:
            command = input("\n> ").strip()
            
            if command == "quit":
                break
            
            elif command.startswith("class "):
                class_name = command[6:].strip()
                analyzer.find_by_class(class_name)
            
            elif command.startswith("id "):
                element_id = command[3:].strip()
                analyzer.find_by_id(element_id)
            
            elif command.startswith("tag "):
                tag_name = command[4:].strip()
                results = analyzer.find_by_tag(tag_name)
                print(f"[DEBUG] {len(results)}å€‹è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            
            elif command.startswith("css "):
                selector = command[4:].strip()
                analyzer.find_by_css_selector(selector)
            
            elif command == "text":
                text = analyzer.extract_text()
                print(f"\n{text[:500]}...")  # æœ€åˆã®500æ–‡å­—
            
            else:
                print("[DEBUG] ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰")
        
        print("\n" + "=" * 70)
        print("âœ… è§£æå®Œäº†")
        print("=" * 70)
        logger.info("ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†")
    
    except KeyboardInterrupt:
        print("\n\n[DEBUG] ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        logger.info("ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸­æ–­")
    
    except Exception as e:
        print(f"\n[DEBUG] âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)


if __name__ == "__main__":
    main()